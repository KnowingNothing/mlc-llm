





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API (Chat Module) &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_engine.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API (Chat Module)</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python_chat_module.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api-chat-module">
<span id="deploy-python-chat-module"></span><h1>Python API (Chat Module)<a class="headerlink" href="#python-api-chat-module" title="Permalink to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>❗ The Python API with <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> introduced in this page will be
deprecated in the near future.
Please go to <a class="reference internal" href="python_engine.html#deploy-python-engine"><span class="std std-ref">Python API</span></a> for the latest Python API with complete
OpenAI API support.</p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#verify-installation" id="id1">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#run-mlc-models-w-python" id="id2">Run MLC Models w/ Python</a></p></li>
<li><p><a class="reference internal" href="#configure-mlcchat-in-python" id="id3">Configure MLCChat in Python</a></p></li>
<li><p><a class="reference internal" href="#raw-text-generation-in-python" id="id4">Raw Text Generation in Python</a></p></li>
<li><p><a class="reference internal" href="#stream-iterator-in-python" id="id5">Stream Iterator in Python</a></p></li>
<li><p><a class="reference internal" href="#api-reference" id="id6">API Reference</a></p></li>
</ul>
</nav>
<p>We expose ChatModule Python API for the MLC-LLM for easy integration into other Python projects.</p>
<p>The Python API is a part of the MLC-LLM package, which we have prepared pre-built pip wheels via
the <a class="reference internal" href="../install/mlc_llm.html"><span class="doc">installation page</span></a>.</p>
<p>Instead of following this page, you could also checkout the following tutorials in
Python notebook (all runnable in Colab):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_chat_module_getting_started.ipynb">Getting Started with MLC-LLM</a>:
how to quickly download prebuilt models and chat with it</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/notebooks/blob/main/mlc-llm/tutorial_raw_text_generation.ipynb">Raw Text Generation with MLC-LLM</a>:
how to perform raw text generation with MLC-LLM in Python</p></li>
</ul>
<section id="verify-installation">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_llm import ChatModule; print(ChatModule)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the information about the <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> class.</p>
<p>If the command above results in error, follow <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">Install MLC LLM Python Package</span></a> (either install the prebuilt pip wheels
or <a class="reference internal" href="../install/mlc_llm.html#mlcchat-build-from-source"><span class="std std-ref">Option 2. Build from Source</span></a>).</p>
</section>
<section id="run-mlc-models-w-python">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Run MLC Models w/ Python</a><a class="headerlink" href="#run-mlc-models-w-python" title="Permalink to this heading">¶</a></h2>
<p>To run a model with MLC LLM in any platform/runtime, you need:</p>
<ol class="arabic simple">
<li><p><strong>Model weights</strong> converted to MLC format (e.g. <a class="reference external" href="https://huggingface.co/mlc-ai/RedPajama-INCITE-Chat-3B-v1-MLC/tree/main">RedPajama-INCITE-Chat-3B-v1-MLC</a>.)</p></li>
<li><p><strong>Model library</strong> that comprises the inference logic (see repo <a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">binary-mlc-llm-libs</a>).</p></li>
</ol>
<p>There are two ways to obtain the model weights and libraries:</p>
<ol class="arabic simple">
<li><p>Compile your own model weights and libraries following <a class="reference internal" href="../compilation/compile_models.html"><span class="doc">the model compilation page</span></a>.</p></li>
<li><p>Use off-the-shelf <a class="reference external" href="https://huggingface.co/mlc-ai">prebuilt models weights</a> and
<a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">prebuilt model libraries</a> (see <a class="reference internal" href="../prebuilt_models.html#model-prebuilts"><span class="std std-ref">Model Prebuilts</span></a> for details).</p></li>
</ol>
<p>We use off-the-shelf prebuilt models in this page. However, same steps apply if you want to run
the models you compiled yourself.</p>
<p><strong>Step 1: Download prebuilt model weights and libraries</strong></p>
<p>Skip this step if you have already obtained the model weights and libraries.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Activate your conda environment</span>
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>git-lfs

<span class="c1"># Download pre-conveted weights</span>
git<span class="w"> </span>lfs<span class="w"> </span>install<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>mkdir<span class="w"> </span>dist/
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/Llama-2-7b-chat-hf-q4f16_1-MLC<span class="w"> </span><span class="se">\</span>
<span class="w">                                 </span>dist/Llama-2-7b-chat-hf-q4f16_1-MLC

<span class="c1"># Download pre-compiled model library</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt_libs
</pre></div>
</div>
<p><strong>Step 2: Run the model in Python</strong></p>
<p>Use the conda environment you used to install <code class="docutils literal notranslate"><span class="pre">mlc_llm</span></code>.
From the <code class="docutils literal notranslate"><span class="pre">mlc-llm</span></code> directory, you can create a Python
file <code class="docutils literal notranslate"><span class="pre">sample_mlc_llm.py</span></code> and paste the following lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>

<span class="c1"># You can change to other models that you downloaded</span>
<span class="c1"># Model variants of the same architecture can reuse the same model library</span>
<span class="c1"># Here WizardMath reuses Mistral&#39;s model library</span>
<span class="c1"># cm = ChatModule(</span>
<span class="c1">#     model=&quot;dist/Mistral-7B-Instruct-v0.2-q4f16_1-MLC&quot;,  # or &quot;dist/WizardMath-7B-V1.1-q4f16_1-MLC&quot;</span>
<span class="c1">#     model_lib_path=&quot;dist/prebuilt_libs/Mistral-7B-Instruct-v0.2/Mistral-7B-Instruct-v0.2-q4f16_1-cuda.so&quot;</span>
<span class="c1"># )</span>

<span class="c1"># Generate a response for a given prompt</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Reset the chat module by</span>
<span class="c1"># cm.reset_chat()</span>
</pre></div>
</div>
<p>Now run the Python file to start the chat</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>sample_mlc_llm.py
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Thank you for your question! The meaning of life is a complex and subjective topic that has been debated by philosophers, theologians, scientists, and many others for centuries. There is no one definitive answer to this question, as it can vary depending on a person&#39;s beliefs, values, experiences, and perspectives.

However, here are some possible ways to approach the question:

1. Religious or spiritual beliefs: Many people believe that the meaning of life is to fulfill a divine or spiritual purpose, whether that be to follow a set of moral guidelines, to achieve spiritual enlightenment, or to fulfill a particular destiny.
2. Personal growth and development: Some people believe that the meaning of life is to learn, grow, and evolve as individuals, to develop one&#39;s talents and abilities, and to become the best version of oneself.
3. Relationships and connections: Others believe that the meaning of life is to form meaningful connections and relationships with others, to love and be loved, and to build a supportive and fulfilling social network.
4. Contribution and impact: Some people believe that the meaning of life is to make a positive impact on the world, to contribute to society in a meaningful way, and to leave a lasting legacy.
5. Simple pleasures and enjoyment: Finally, some people believe that the meaning of life is to simply enjoy the present moment, to find pleasure and happiness in the simple things in life, and to appreciate the beauty and wonder of the world around us.

Ultimately, the meaning of life is a deeply personal and subjective question, and each person must find their own answer based on their own beliefs, values, and experiences.

Statistics: prefill: 3477.5 tok/s, decode: 153.6 tok/s

I listed out 5 possible ways to approach the question of the meaning of life.
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Running other models</strong></p>
<p>Checkout the <a class="reference internal" href="../prebuilt_models.html"><span class="doc">Model Prebuilts</span></a> page to run other pre-compiled models.</p>
<p>For models other than the prebuilt ones we provided:</p>
<ol class="arabic simple">
<li><p>If the model is a variant to an existing model library (e.g. <code class="docutils literal notranslate"><span class="pre">WizardMathV1.1</span></code> and <code class="docutils literal notranslate"><span class="pre">OpenHermes</span></code> are variants of <code class="docutils literal notranslate"><span class="pre">Mistral</span></code> as
shown in the code snippet), follow <a class="reference internal" href="../compilation/convert_weights.html#convert-weights-via-mlc"><span class="std std-ref">Convert Weights via MLC</span></a> to convert the weights and reuse existing model libraries.</p></li>
<li><p>Otherwise, follow <a class="reference internal" href="../compilation/compile_models.html#compile-model-libraries"><span class="std std-ref">Compile Model Libraries</span></a> to compile both the model library and weights.</p></li>
</ol>
</section>
<section id="configure-mlcchat-in-python">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Configure MLCChat in Python</a><a class="headerlink" href="#configure-mlcchat-in-python" title="Permalink to this heading">¶</a></h2>
<p>If you have checked out <a class="reference internal" href="mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>, you would know
that you could configure MLCChat through various fields such as <code class="docutils literal notranslate"><span class="pre">temperature</span></code>. We provide the
option of overriding any field you’d like in Python, so that you do not need to manually edit
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p>
<p>Since there are two concepts – <cite>MLCChat Configuration</cite> and <cite>Conversation Configuration</cite> – we correspondingly
provide two dataclasses <a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.ChatConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatConfig</span></code></a> and <a class="reference internal" href="#mlc_llm.ConvConfig" title="mlc_llm.ConvConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ConvConfig</span></code></a>.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Using a `ConvConfig`, we modify `system`, a field in the conversation template</span>
<span class="c1"># `system` refers to the prompt encoded before starting the chat</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system_message</span><span class="o">=</span><span class="s1">&#39;Please show as much happiness as you can when talking to me.&#39;</span><span class="p">)</span>

<span class="c1"># We then include the `ConvConfig` instance in `ChatConfig` while overriding `max_gen_len`</span>
<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">,</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># You could also pass in a `ConvConfig` instance to `reset_chat()`</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">system</span><span class="o">=</span><span class="s1">&#39;Please show as much sadness as you can when talking to me.&#39;</span><span class="p">)</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">max_gen_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">)</span>
<span class="n">cm</span><span class="o">.</span><span class="n">reset_chat</span><span class="p">(</span><span class="n">chat_config</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is one plus one?&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<details class="summary-see-output">
<summary>See output</summary><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Using model folder: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1
Using mlc chat config: ./dist/prebuilt/mlc-chat-Llama-2-7b-chat-hf-q4f16_1/mlc-chat-config.json
Using library model: ./dist/prebuilt/lib/Llama-2-7b-chat-hf-q4f16_1-cuda.so

Oh, wow, *excitedly* one plus one? *grinning* Well, let me see... *counting on fingers* One plus one is... *eureka* Two!
...

*Sobs* Oh, the tragedy of it all... *sobs* One plus one... *chokes back tears* It&#39;s... *gulps* it&#39;s... *breaks down in tears* TWO!
...
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You do not need to specify the entire <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> or <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code>. Instead, we will first
load all the fields defined in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, a file required when instantiating
a <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a>. Then, we will load in the optional <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> you provide, overriding the
fields specified.</p>
<p>It is also worth noting that <code class="docutils literal notranslate"><span class="pre">ConvConfig</span></code> itself is overriding the original conversation template
specified by the field <code class="docutils literal notranslate"><span class="pre">conv_template</span></code> in the chat configuration. Learn more about it in
<a class="reference internal" href="mlc_chat_config.html#configure-mlc-chat-json"><span class="std std-ref">Configure MLCChat in JSON</span></a>.</p>
</div>
</section>
<section id="raw-text-generation-in-python">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Raw Text Generation in Python</a><a class="headerlink" href="#raw-text-generation-in-python" title="Permalink to this heading">¶</a></h2>
<p>Raw text generation allows the user to have more flexibility over his prompts,
without being forced to create a new conversational template, making prompt customization easier.
This serves other demands for APIs to handle LLM generation without the usual system prompts and other items.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">ChatConfig</span><span class="p">,</span> <span class="n">ConvConfig</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Use a `ConvConfig` to define the generation settings</span>
<span class="c1"># Since the &quot;LM&quot; template only supports raw text generation,</span>
<span class="c1"># System prompts will not be executed even if provided</span>
<span class="n">conv_config</span> <span class="o">=</span> <span class="n">ConvConfig</span><span class="p">(</span><span class="n">stop_tokens</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,],</span> <span class="n">add_bos</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stop_str</span><span class="o">=</span><span class="s2">&quot;[INST]&quot;</span><span class="p">)</span>

<span class="c1"># Note that `conv_config` is an optional subfield of `chat_config`</span>
<span class="c1"># The &quot;LM&quot; template serves the basic purposes of raw text generation</span>
<span class="n">chat_config</span> <span class="o">=</span> <span class="n">ChatConfig</span><span class="p">(</span><span class="n">conv_config</span><span class="o">=</span><span class="n">conv_config</span><span class="p">,</span> <span class="n">conv_template</span><span class="o">=</span><span class="s2">&quot;LM&quot;</span><span class="p">)</span>

<span class="c1"># Using the `chat_config` we created, instantiate a `ChatModule`</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">chat_config</span><span class="o">=</span><span class="n">chat_config</span><span class="p">,</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>
<span class="c1"># To make the model follow conversations a chat structure should be provided</span>
<span class="c1"># This allows users to build their own prompts without building a new template</span>
<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&lt;&lt;SYS&gt;&gt;</span><span class="se">\n</span><span class="s2">You are a helpful, respectful and honest assistant.</span><span class="se">\n</span><span class="s2">&lt;&lt;/SYS&gt;&gt;</span><span class="se">\n\n</span><span class="s2">&quot;</span>
<span class="n">inst_prompt</span> <span class="o">=</span> <span class="s2">&quot;What is mother nature?&quot;</span>

<span class="c1"># Concatenate system and instruction prompts, and add instruction tags</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;[INST] </span><span class="si">{</span><span class="n">system_prompt</span><span class="o">+</span><span class="n">inst_prompt</span><span class="si">}</span><span class="s2"> [/INST]&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># The LM template has no memory, so it will be reset every single generation</span>
<span class="c1"># In this case the model will just follow normal text completion</span>
<span class="c1"># because there isn&#39;t a chat structure</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
   <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;Life is a quality that distinguishes&quot;</span><span class="p">,</span>
   <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">LM</span></code> is a template without memory, which means that every execution will be cleared.
Additionally, system prompts will not be run when instantiating a <cite>mlc_llm.ChatModule</cite>,
unless explicitly given inside the prompt.</p>
</div>
</section>
<section id="stream-iterator-in-python">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">Stream Iterator in Python</a><a class="headerlink" href="#stream-iterator-in-python" title="Permalink to this heading">¶</a></h2>
<p>Stream Iterator gives users an option to stream generated text to the function that the API is called from,
instead of streaming to stdout, which could be a necessity when building services on top of MLC Chat.</p>
<p>We provide an example below.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamIterator</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span>
   <span class="n">model</span><span class="o">=</span><span class="s2">&quot;dist/Llama-2-7b-chat-hf-q4f16_1-MLC&quot;</span><span class="p">,</span>
   <span class="n">model_lib_path</span><span class="o">=</span><span class="s2">&quot;dist/prebuilt_libs/Llama-2-7b-chat-hf/Llama-2-7b-chat-hf-q4f16_1-cuda.so&quot;</span>
   <span class="c1"># Vulkan on Linux: Llama-2-7b-chat-hf-q4f16_1-vulkan.so</span>
   <span class="c1"># Metal on macOS: Llama-2-7b-chat-hf-q4f16_1-metal.so</span>
   <span class="c1"># Other platforms: Llama-2-7b-chat-hf-q4f16_1-{backend}.{suffix}</span>
<span class="p">)</span>

<span class="c1"># Stream to an Iterator</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>

<span class="n">stream</span> <span class="o">=</span> <span class="n">StreamIterator</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">generation_thread</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span>
   <span class="n">target</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">,</span>
   <span class="n">kwargs</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;prompt&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span> <span class="s2">&quot;progress_callback&quot;</span><span class="p">:</span> <span class="n">stream</span><span class="p">},</span>
<span class="p">)</span>
<span class="n">generation_thread</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">output</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
<span class="k">for</span> <span class="n">delta_message</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
   <span class="n">output</span> <span class="o">+=</span> <span class="n">delta_message</span>

<span class="n">generation_thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="api-reference">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h2>
<p>User can initiate a chat module by creating <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> class, which is a wrapper of the MLC-LLM model.
The <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> class provides the following methods:</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlc_llm.ChatModule">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_llm.</span></span><span class="sig-name descname"><span class="pre">ChatModule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.chat_module.ChatConfig"><span class="pre">ChatConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.ChatModule" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>The ChatModule for MLC LLM.</p>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span>
<span class="kn">from</span> <span class="nn">mlc_llm.callback</span> <span class="kn">import</span> <span class="n">StreamToStdout</span>

<span class="c1"># Create a ChatModule instance</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>

<span class="c1"># Generate a response for a given prompt</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Print prefill and decode performance statistics</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
    <span class="n">prompt</span><span class="o">=</span><span class="s2">&quot;How many points did you list out?&quot;</span><span class="p">,</span>
    <span class="n">progress_callback</span><span class="o">=</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>str</em>) – The model folder after compiling with MLC-LLM build process. The parameter
can either be the model name with its quantization scheme
(e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1</span></code>), or a full path to the model
folder. In the former case, we will use the provided name to search
for the model folder over possible paths.</p></li>
<li><p><strong>device</strong> (<em>str</em>) – The description of the device to run on. User should provide a string in the
form of ‘device_name:device_id’ or ‘device_name’, where ‘device_name’ is one of
‘cuda’, ‘metal’, ‘vulkan’, ‘rocm’, ‘opencl’, ‘auto’ (automatically detect the
local device), and ‘device_id’ is the device id to run on. If no ‘device_id’
is provided, it will be set to 0 by default.</p></li>
<li><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.ChatConfig"><em>ChatConfig</em></a><em>]</em>) – A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. Will be used to override the
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>.</p></li>
<li><p><strong>model_lib_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).
If unspecified, we will use the provided <code class="docutils literal notranslate"><span class="pre">model</span></code> to search over
possible paths.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.ChatModule.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.chat_module.ChatConfig"><span class="pre">ChatConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.ChatModule.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.ChatModule.benchmark_generate">
<span class="sig-name descname"><span class="pre">benchmark_generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generate_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_llm.ChatModule.benchmark_generate" title="Permalink to this definition">¶</a></dt>
<dd><p>Controlled generation with input prompt and fixed number of
generated tokens, ignoring system prompt. For example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span>

<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;Llama-2-7b-chat-hf-q4f16_1&quot;</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">benchmark_generate</span><span class="p">(</span><span class="s2">&quot;What&#39;s the meaning of life?&quot;</span><span class="p">,</span> <span class="n">generate_length</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated text:</span><span class="se">\n</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Statistics: </span><span class="si">{</span><span class="n">cm</span><span class="o">.</span><span class="n">stats</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>will generate 256 tokens in total based on prompt “What’s the meaning
of life?”. After generation, you can use <cite>cm.stats()</cite> to print the
generation speed.</p>
<p class="rubric">Notes</p>
<p>1. This function is typically used in controlled benchmarks. It generates
text without system prompt (i.e., it is pure text generation with no chat
style) and ignores the token stop model(s).
2. To make the benchmark as accurate as possible, we first do a round of
warmup prefill and decode before text generation.
3. This function resets the previous performance statistics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>str</em>) – The prompt of the text generation.</p></li>
<li><p><strong>generate_length</strong> (<em>int</em>) – The target length of generation.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> – The generated text output.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.ChatModule.embed_text">
<span class="sig-name descname"><span class="pre">embed_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.ChatModule.embed_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a text input, returns its embedding in the LLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>input</strong> (<em>str</em>) – The user input string.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>embedding</strong> – The embedding of the text.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>tvm.runtime.NDArray</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a high-level method and is only used for retrieving text embeddings. Users are
not supposed to call <a class="reference internal" href="#mlc_llm.ChatModule.generate" title="mlc_llm.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> after calling this method in the same chat session,
since the input to this method is not prefilled and will cause error. If user needs to
call <a class="reference internal" href="#mlc_llm.ChatModule.generate" title="mlc_llm.ChatModule.generate"><code class="xref py py-func docutils literal notranslate"><span class="pre">generate()</span></code></a> later, please call <a class="reference internal" href="#mlc_llm.ChatModule.reset_chat" title="mlc_llm.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a> first.
For a more fine-grained embedding API, see <code class="xref py py-func docutils literal notranslate"><span class="pre">_embed()</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.ChatModule.generate">
<span class="sig-name descname"><span class="pre">generate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prompt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">ChatCompletionMessage</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">generation_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_llm.GenerationConfig" title="mlc_llm.chat_module.GenerationConfig"><span class="pre">GenerationConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">progress_callback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stateless</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#mlc_llm.ChatModule.generate" title="Permalink to this definition">¶</a></dt>
<dd><p>A high-level method that returns the full response from the chat module given a user
prompt. User can optionally specify which callback method to use upon receiving the
response. By default, no callback will be applied.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prompt</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>ChatCompletionMessage</em><em>]</em><em>]</em>) – <p>The user input prompt, i.e. a question to ask the chat module.
It can also be the whole conversation history (list of messages with role and content)
eg:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span>
    <span class="n">ChatCompletionMessage</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Hello, how are you?&quot;</span><span class="p">),</span>
    <span class="n">ChatCompletionMessage</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span> \
        <span class="n">content</span><span class="o">=</span><span class="s2">&quot;I&#39;m fine, thank you. How about you?&quot;</span><span class="p">),</span>
    <span class="n">ChatCompletionMessage</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;I&#39;m good too.&quot;</span><span class="p">),</span>
<span class="p">]</span>
</pre></div>
</div>
</p></li>
<li><p><strong>generation_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_llm.GenerationConfig" title="mlc_llm.GenerationConfig"><em>GenerationConfig</em></a><em>]</em>) – The generation config object to override the ChatConfig generation settings.</p></li>
<li><p><strong>progress_callback</strong> (<em>object</em>) – The optional callback method used upon receiving a newly generated message from the
chat module. See <cite>mlc_llm/callback.py</cite> for a full list of available callback classes.
Currently, only streaming to stdout callback method is supported, see <cite>Examples</cite> for
more detailed usage.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>output</strong> – The generated full output from the chat module.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Suppose we would like to stream the response of the chat module to stdout</span>
<span class="c1"># with a refresh interval of 2. Upon calling generate(), We will see the response of</span>
<span class="c1"># the chat module streaming to stdout piece by piece, and in the end we receive the</span>
<span class="c1"># full response as a single string `output`.</span>

<span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">ChatModule</span><span class="p">,</span> <span class="n">GenerationConfig</span><span class="p">,</span> <span class="n">callback</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">ChatModule</span><span class="p">(</span><span class="n">xxx</span><span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;what&#39;s the color of banana?&quot;</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
  <span class="n">prompt</span><span class="p">,</span> <span class="n">GenerationConfig</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mf">0.8</span><span class="p">),</span> <span class="n">callback</span><span class="o">.</span><span class="n">StreamToStdout</span><span class="p">(</span><span class="n">callback_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.ChatModule.reset_chat">
<span class="sig-name descname"><span class="pre">reset_chat</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">chat_config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.chat_module.ChatConfig"><span class="pre">ChatConfig</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.ChatModule.reset_chat" title="Permalink to this definition">¶</a></dt>
<dd><p>Reset the chat session, clear all chat history, and potentially
override the original <cite>mlc-chat-config.json</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>chat_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.ChatConfig"><em>ChatConfig</em></a><em>]</em>) – A <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> instance partially filled. If specified, the chat
module will reload the <cite>mlc-chat-config.json</cite>, and override it with
<code class="docutils literal notranslate"><span class="pre">chat_config</span></code>, just like in initialization.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The model remains the same after <a class="reference internal" href="#mlc_llm.ChatModule.reset_chat" title="mlc_llm.ChatModule.reset_chat"><code class="xref py py-func docutils literal notranslate"><span class="pre">reset_chat()</span></code></a>.
To reload module, please either re-initialize a <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChatModule</span></code></a> instance
or use <code class="xref py py-func docutils literal notranslate"><span class="pre">_reload()</span></code> instead.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.ChatModule.stats">
<span class="sig-name descname"><span class="pre">stats</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">str</span></span></span><a class="headerlink" href="#mlc_llm.ChatModule.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the runtime stats of the encoding step, decoding step (and embedding step if exists)
of the chat module in text form.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stats</strong> – The runtime stats text.</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_llm.ChatConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_llm.</span></span><span class="sig-name descname"><span class="pre">ChatConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model_lib:</span> <span class="pre">~typing.Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">local_id:</span> <span class="pre">~typing.Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_template:</span> <span class="pre">~typing.Optional[~typing.Union[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~mlc_llm.protocol.conversation_protocol.Conversation]]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">presence_penalty:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency_penalty:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repetition_penalty:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_gen_len:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shift_fill_factor:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tokenizer_files:</span> <span class="pre">~typing.Optional[~typing.List[str]]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">conv_config:</span> <span class="pre">~typing.Optional[~mlc_llm.chat_module.ConvConfig]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_category:</span> <span class="pre">~typing.Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_name:</span> <span class="pre">~typing.Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_parallel_shards:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_presharded_weights:</span> <span class="pre">~typing.Optional[bool]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context_window_size:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sliding_window_size:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_chunk_size:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attention_sink_size:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt:</span> <span class="pre">~typing.Optional[str]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs:</span> <span class="pre">~typing.Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">=</span> <span class="pre">&lt;factory&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.ChatConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>A dataclass that represents user-defined partial configuration for the
chat config file.</p>
<p>An instance of <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> can be passed in to the instantiation of a
<a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> instance to override the default setting in
<code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> under the model folder.</p>
<p>Since the configuration is partial, everything will be <code class="docutils literal notranslate"><span class="pre">Optional</span></code>.</p>
<p>Note that we will exploit this class to also represent <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>
during intermediate processing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model_lib</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The necessary model library to launch this model architecture. We recommend
reuse model library when possible. For example, all LLaMA-7B models can
use <code class="docutils literal notranslate"><span class="pre">vicuna-v1-7b-{matching</span> <span class="pre">quantization</span> <span class="pre">scheme}</span></code>. So you can distribute
LLaMA-7B weight variants and still use them in prebuilt MLC chat apps.</p></li>
<li><p><strong>local_id</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Uniquely identifying the model in application. This is also used by
command line interface app to specify which model to run.</p></li>
<li><p><strong>conv_template</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The name of the conversation template that this chat uses.</p></li>
<li><p><strong>temperature</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – The temperature applied to logits before sampling. The default value is
<code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a
lower temperature produces more deterministic outputs.</p></li>
<li><p><strong>repetition_penalty</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – <p>The repetition penalty controls the likelihood of the model generating
repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no
repetition penalty is applied. Increasing the value reduces the
likelihood of repeat text generation. However, setting a high
<code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless
texts. The ideal choice of repetition penalty may vary among models.</p>
<p>For more details on how repetition penalty controls text generation, please
check out the CTRL paper (<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>).</p>
</p></li>
<li><p><strong>top_p</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – <p>This parameter determines the set of tokens from which we sample during
decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select
tokens from the minimal set that has a cumulative probability exceeding
the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this blog
post: <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling</a>.</p>
</p></li>
<li><p><strong>mean_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The approximated average number of generated tokens in each round. Used
to determine whether the maximum window size would be exceeded.</p></li>
<li><p><strong>max_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The maximum number of tokens to be generated in each round. Would simply
stop generating after this number is exceeded.</p></li>
<li><p><strong>shift_fill_factor</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – The fraction of maximum window size to shift when it is exceeded.</p></li>
<li><p><strong>tokenizer_files</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – List of tokenizer files of the model.</p></li>
<li><p><strong>conv_config</strong> (<em>Optional</em><em>[</em><a class="reference internal" href="#mlc_llm.ConvConfig" title="mlc_llm.ConvConfig"><em>ConvConfig</em></a><em>]</em>) – The partial overriding configuration for conversation template. Will first
load the predefined template with the name specified in <code class="docutils literal notranslate"><span class="pre">conv_template</span></code>
and then override some of the configurations specified in <code class="docutils literal notranslate"><span class="pre">conv_config</span></code>.</p></li>
<li><p><strong>model_category</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The category of the model’s architecture (e.g. <code class="docutils literal notranslate"><span class="pre">llama</span></code>, <code class="docutils literal notranslate"><span class="pre">gpt_neox</span></code>, <code class="docutils literal notranslate"><span class="pre">rwkv</span></code>).</p></li>
<li><p><strong>model_name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Name of the model (e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf</span></code>).</p></li>
<li><p><strong>tensor_parallel_shards</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Tensor parallel degree.</p></li>
<li><p><strong>use_presharded_weights</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – If True, the weights were saved with sharding already applied.</p></li>
<li><p><strong>context_window_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – Maximum kv cache window size.</p></li>
<li><p><strong>prefill_chunk_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – (Experimental) The chunk size during prefilling. By default,
the chunk size is the same as sliding window or max sequence length.
This flag subjects to future refactoring.</p></li>
<li><p><strong>attention_sink_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – (Experimental) The number of stored sinks. Only supported on Mistral yet. By default,
the number of sinks is 4. This flag subjects to future refactoring.</p></li>
<li><p><strong>sliding_window_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – (Experimental) The sliding window size in sliding window attention (SWA).
This optional field overrides the <cite>sliding_window_size</cite> in config.json for
those models that use SWA. Currently only useful when compiling Mistral.
This flag subjects to future refactoring.</p></li>
<li><p><strong>opt</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Optimization flags. MLC LLM maintains a predefined set of optimization flags,
denoted as O0, O1, O2, O3, where O0 means no optimization, O2 means majority of them,
and O3 represents extreme optimization that could potentially break the system.
Meanwhile, optimization flags could be explicitly specified via details knobs, e.g.
–opt=”cublas_gemm=1;cudagraph=0”.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_llm.ConvConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_llm.</span></span><span class="sig-name descname"><span class="pre">ConvConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">system_template</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">system_message</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">system_prefix_token_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">role_templates</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">messages</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">seps</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">role_content_sep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">role_empty_sep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_str</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_token_ids</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">function_string</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_function_calling</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.ConvConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>A dataclass that represents user-defined partial configuration for conversation template.</p>
<p>This is an attribute of <a class="reference internal" href="#mlc_llm.ChatConfig" title="mlc_llm.ChatConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatConfig</span></code></a>, which can then be passed in to the
instantiation of a <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> instance to override the default
setting in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> under the model folder. Note that we will
first load the predefined template with the name specified in <code class="docutils literal notranslate"><span class="pre">conv_template</span></code>.</p>
<p>Since the configuration is partial, everything will be <code class="docutils literal notranslate"><span class="pre">Optional</span></code>.</p>
<p>The parameters are the same as <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.protocol.conversation_protocol.Conversation</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – Name of the conversation.</p></li>
<li><p><strong>system_template</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The system prompt template, it optionally contains the system
message placeholder, and the placeholder will be replaced with
the system message below.</p></li>
<li><p><strong>system_message</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The content of the system prompt (without the template format).</p></li>
<li><p><strong>system_prefix_token_ids</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) – The system token ids to be prepended at the beginning of tokenized
generated prompt.</p></li>
<li><p><strong>roles</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em>) – The conversation roles</p></li>
<li><p><strong>role_templates</strong> (<em>Optional</em><em>[</em><em>Dict</em><em>[</em><em>str</em><em>, </em><em>str</em><em>]</em><em>]</em>) – The roles prompt template, it optionally contains the defaults
message placeholders and will be replaced by actual content</p></li>
<li><p><strong>messages</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>Tuple</em><em>[</em><em>str</em><em>, </em><em>Optional</em><em>[</em><em>str</em><em>]</em><em>]</em><em>]</em><em>]</em>) – The conversation history messages.
Each message is a pair of strings, denoting “(role, content)”.
The content can be None.</p></li>
<li><p><strong>seps</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – An array of strings indicating the separators to be used after a user
message and a model message respectively.</p></li>
<li><p><strong>role_content_sep</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The separator between the role and the content in a message.</p></li>
<li><p><strong>role_empty_sep</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The separator between the role and empty contents.</p></li>
<li><p><strong>stop_str</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – When the <code class="docutils literal notranslate"><span class="pre">stop_str</span></code> is encountered, the model will stop generating output.</p></li>
<li><p><strong>stop_token_ids</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>int</em><em>]</em><em>]</em>) – A list of token IDs that act as stop tokens.</p></li>
<li><p><strong>function_string</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The function calling string.</p></li>
<li><p><strong>use_function_calling</strong> (<em>Optional</em><em>[</em><em>bool</em><em>]</em>) – Whether using function calling or not, helps check for output message format in API call.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_llm.GenerationConfig">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_llm.</span></span><span class="sig-name descname"><span class="pre">GenerationConfig</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">temperature:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">repetition_penalty:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_p:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mean_gen_len:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_gen_len:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">presence_penalty:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency_penalty:</span> <span class="pre">~typing.Optional[float]</span> <span class="pre">=</span> <span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n:</span> <span class="pre">~typing.Optional[int]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop:</span> <span class="pre">~typing.Optional[~typing.Union[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.List[str]]]</span> <span class="pre">=</span> <span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kwargs:</span> <span class="pre">~typing.Dict[str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">~typing.Any]</span> <span class="pre">=</span> <span class="pre">&lt;factory&gt;</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.GenerationConfig" title="Permalink to this definition">¶</a></dt>
<dd><p>A dataclass that represents user-defined generation configuration.</p>
<p>An instance of <code class="docutils literal notranslate"><span class="pre">GenerationConfig</span></code> can be passed in to the generate function
of a <a class="reference internal" href="#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a> instance to override the default generation
setting in <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code> and <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> under the model folder.</p>
<p>Once the generation ends, <code class="docutils literal notranslate"><span class="pre">GenerationConfig</span></code> is discarded, since the values
will only override the <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> generation settings during one generation,
unless it is recurrently passed to generate function. This allows changing generation
settings over time, without overriding <code class="docutils literal notranslate"><span class="pre">ChatConfig</span></code> permanently.</p>
<p>Since the configuraiton is partial, everything will be <code class="docutils literal notranslate"><span class="pre">Optional</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>temperature</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – The temperature applied to logits before sampling. The default value is
<code class="docutils literal notranslate"><span class="pre">0.7</span></code>. A higher temperature encourages more diverse outputs, while a
lower temperature produces more deterministic outputs.</p></li>
<li><p><strong>presence_penalty</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – Number between -2.0 and 2.0. Positive values penalize new tokens based on
whether they appear in the text so far, increasing the model’s likelihood
to talk about new topics. Negative values can increase the likelihood of
repetition.</p></li>
<li><p><strong>frequency_penalty</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – Number between -2.0 and 2.0. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model’s likelihood to
repeat the same line verbatim. Negative values can increase the likelihood of
repetition.</p></li>
<li><p><strong>repetition_penalty</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – <p>The repetition penalty controls the likelihood of the model generating
repeated texts. The default value is set to <code class="docutils literal notranslate"><span class="pre">1.0</span></code>, indicating that no
repetition penalty is applied. Increasing the value reduces the
likelihood of repeat text generation. However, setting a high
<code class="docutils literal notranslate"><span class="pre">repetition_penalty</span></code> may result in the model generating meaningless
texts. The ideal choice of repetition penalty may vary among models. Only
Active when presence_penalty and frequency_penalty are both 0.0.</p>
<p>For more details on how repetition penalty controls text generation, please
check out the CTRL paper (<a class="reference external" href="https://arxiv.org/pdf/1909.05858.pdf">https://arxiv.org/pdf/1909.05858.pdf</a>).</p>
</p></li>
<li><p><strong>top_p</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – <p>This parameter determines the set of tokens from which we sample during
decoding. The default value is set to <code class="docutils literal notranslate"><span class="pre">0.95</span></code>. At each step, we select
tokens from the minimal set that has a cumulative probability exceeding
the <code class="docutils literal notranslate"><span class="pre">top_p</span></code> parameter.</p>
<p>For additional information on top-p sampling, please refer to this blog
post: <a class="reference external" href="https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling">https://huggingface.co/blog/how-to-generate#top-p-nucleus-sampling</a>.</p>
</p></li>
<li><p><strong>mean_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The approximated average number of generated tokens in each round. Used
to determine whether the maximum window size would be exceeded.</p></li>
<li><p><strong>max_gen_len</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – This parameter determines the maximum length of the generated text. If it is
not set, the model will generate text until it encounters a stop token.</p></li>
<li><p><strong>n</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – This parameter determines the number of text samples to generate. The default
value is <code class="docutils literal notranslate"><span class="pre">1</span></code>. Note that this parameter is only used when <code class="docutils literal notranslate"><span class="pre">stream</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>stop</strong> (<em>Optional</em><em>[</em><em>Union</em><em>[</em><em>str</em><em>, </em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em><em>]</em>) – When <code class="docutils literal notranslate"><span class="pre">stop</span></code> is encountered, the model will stop generating output.
It can be a string or a list of strings. If it is a list of strings, the model
will stop generating output when any of the strings in the list is encountered.
Note that this parameter does not override the default stop string of the model.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>


           </div>
           
          </div>
          

<footer>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>