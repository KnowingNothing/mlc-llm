





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Python API &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="iOS App and Swift API" href="ios.html" />
    <link rel="prev" title="CLI" href="cli.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../get_started/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/introduction.html">Introduction to MLC LLM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="javascript.html">WebLLM and JavaScript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="rest.html">REST API</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli.html">CLI</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Python API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#verify-installation">Verify Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-llmengine">Run LLMEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#run-asyncllmengine">Run AsyncLLMEngine</a></li>
<li class="toctree-l2"><a class="reference internal" href="#engine-mode">Engine Mode</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-your-own-model-with-python-api">Deploy Your Own Model with Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#api-reference">API Reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#mlc_llm.LLMEngine"><code class="docutils literal notranslate"><span class="pre">LLMEngine</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mlc_llm.LLMEngine.__init__"><code class="docutils literal notranslate"><span class="pre">LLMEngine.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_llm.LLMEngine.abort"><code class="docutils literal notranslate"><span class="pre">LLMEngine.abort()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#mlc_llm.AsyncLLMEngine"><code class="docutils literal notranslate"><span class="pre">AsyncLLMEngine</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mlc_llm.AsyncLLMEngine.__init__"><code class="docutils literal notranslate"><span class="pre">AsyncLLMEngine.__init__()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlc_llm.AsyncLLMEngine.abort"><code class="docutils literal notranslate"><span class="pre">AsyncLLMEngine.abort()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="android.html">Android App</a></li>
<li class="toctree-l1"><a class="reference internal" href="ide_integration.html">Code Completion IDE Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlc_chat_config.html">Customize MLC Config File in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../compilation/convert_weights.html">Convert Weights via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/compile_models.html">Compile Model Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compilation/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Model Prebuilts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../prebuilt_models.html">Model Prebuilts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="../community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Python API</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/deploy/python_engine.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="python-api">
<span id="deploy-python-engine"></span><h1>Python API<a class="headerlink" href="#python-api" title="Permalink to this heading">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This page introduces the Python API with LLMEngine in MLC LLM.
If you want to check out the old Python API which uses <a class="reference internal" href="python_chat_module.html#mlc_llm.ChatModule" title="mlc_llm.ChatModule"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.ChatModule</span></code></a>,
please go to <a class="reference internal" href="python_chat_module.html#deploy-python-chat-module"><span class="std std-ref">Python API (Chat Module)</span></a></p>
</div>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#verify-installation" id="id6">Verify Installation</a></p></li>
<li><p><a class="reference internal" href="#run-llmengine" id="id7">Run LLMEngine</a></p></li>
<li><p><a class="reference internal" href="#run-asyncllmengine" id="id8">Run AsyncLLMEngine</a></p></li>
<li><p><a class="reference internal" href="#engine-mode" id="id9">Engine Mode</a></p></li>
<li><p><a class="reference internal" href="#deploy-your-own-model-with-python-api" id="id10">Deploy Your Own Model with Python API</a></p></li>
<li><p><a class="reference internal" href="#api-reference" id="id11">API Reference</a></p></li>
</ul>
</nav>
<p>MLC LLM provides Python API through classes <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> and <a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a>
which <strong>support full OpenAI API completeness</strong> for easy integration into other Python projects.</p>
<p>This page introduces how to use the LLM engines in MLC LLM.
The Python API is a part of the MLC-LLM package, which we have prepared pre-built pip wheels via
the <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">installation page</span></a>.</p>
<section id="verify-installation">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">Verify Installation</a><a class="headerlink" href="#verify-installation" title="Permalink to this heading">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from mlc_llm import LLMEngine; print(LLMEngine)&quot;</span>
</pre></div>
</div>
<p>You are expected to see the output of <code class="docutils literal notranslate"><span class="pre">&lt;class</span> <span class="pre">'mlc_llm.serve.engine.LLMEngine'&gt;</span></code>.</p>
<p>If the command above results in error, follow <a class="reference internal" href="../install/mlc_llm.html#install-mlc-packages"><span class="std std-ref">Install MLC LLM Python Package</span></a> to install prebuilt pip
packages or build MLC LLM from source.</p>
</section>
<section id="run-llmengine">
<h2><a class="toc-backref" href="#id7" role="doc-backlink">Run LLMEngine</a><a class="headerlink" href="#run-llmengine" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> provides the interface of OpenAI chat completion synchronously.
<a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> does not batch concurrent request due to the synchronous design,
and please use <a class="reference internal" href="#python-engine-async-llm-engine"><span class="std std-ref">AsyncLLMEngine</span></a> for request batching process.</p>
<p><strong>Stream Response.</strong> In <a class="reference internal" href="../get_started/quick_start.html#quick-start"><span class="std std-ref">Quick Start</span></a> and <a class="reference internal" href="../get_started/introduction.html#introduction-to-mlc-llm"><span class="std std-ref">Introduction to MLC LLM</span></a>,
we introduced the basic use of <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">LLMEngine</span>

<span class="c1"># Create engine</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">LLMEngine</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Run chat completion in OpenAI API.</span>
<span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">choice</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">engine</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>
</pre></div>
</div>
<p>This code example first creates an <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> instance with the 8B Llama-3 model.
<strong>We design the Python API</strong> <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> <strong>to align with OpenAI API</strong>,
which means you can use <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> in the same way of using
<a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
for both synchronous and asynchronous generation.</p>
<p><strong>Non-stream Response.</strong> The code example above uses the synchronous chat completion
interface and iterate over all the stream responses.
If you want to run without streaming, you can run</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
and <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI chat completion API</a>
for the complete chat completion interface.</p>
</section>
<section id="run-asyncllmengine">
<span id="python-engine-async-llm-engine"></span><h2><a class="toc-backref" href="#id8" role="doc-backlink">Run AsyncLLMEngine</a><a class="headerlink" href="#run-asyncllmengine" title="Permalink to this heading">¶</a></h2>
<p><a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a> provides the interface of OpenAI chat completion with
asynchronous features.
<strong>We recommend using</strong> <a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a> <strong>to batch concurrent request for better throughput.</strong></p>
<p><strong>Stream Response.</strong> The core use of <a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a> for stream responses is as follows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">async</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="k">await</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
  <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
  <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
  <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">:</span>
      <span class="nb">print</span><span class="p">(</span><span class="n">choice</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<details class="summary-the-collapsed-is-a-complete-runnable-example-of-asyncllmengine-in-python">
<summary>The collapsed is a complete runnable example of AsyncLLMEngine in Python.</summary><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>

<span class="kn">from</span> <span class="nn">mlc_llm.serve</span> <span class="kn">import</span> <span class="n">AsyncLLMEngine</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Write a three-day travel plan to Pittsburgh.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">test_completion</span><span class="p">():</span>
    <span class="c1"># Create engine</span>
    <span class="n">async_engine</span> <span class="o">=</span> <span class="n">AsyncLLMEngine</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>

    <span class="n">num_requests</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompts</span><span class="p">)</span>
    <span class="n">output_texts</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">generate_task</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">response</span> <span class="ow">in</span> <span class="k">await</span> <span class="n">async_engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">output_texts</span><span class="p">:</span>
                <span class="n">output_texts</span><span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="n">output_texts</span><span class="p">[</span><span class="n">response</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">+=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span>

    <span class="n">tasks</span> <span class="o">=</span> <span class="p">[</span><span class="n">asyncio</span><span class="o">.</span><span class="n">create_task</span><span class="p">(</span><span class="n">generate_task</span><span class="p">(</span><span class="n">prompts</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_requests</span><span class="p">)]</span>
    <span class="k">await</span> <span class="n">asyncio</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">)</span>

    <span class="c1"># Print output.</span>
    <span class="k">for</span> <span class="n">request_id</span><span class="p">,</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">output_texts</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Output of request </span><span class="si">{</span><span class="n">request_id</span><span class="si">}</span><span class="s2">:</span><span class="se">\n</span><span class="si">{</span><span class="n">output</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">async_engine</span><span class="o">.</span><span class="n">terminate</span><span class="p">()</span>


<span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">test_completion</span><span class="p">())</span>
</pre></div>
</div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
<p><strong>Non-stream Response.</strong> Similarly, <code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncEngine</span></code> provides the non-stream response
interface.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">engine</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
  <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span><span class="p">}],</span>
  <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
  <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
and <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI chat completion API</a>
for the complete chat completion interface.</p>
</section>
<section id="engine-mode">
<h2><a class="toc-backref" href="#id9" role="doc-backlink">Engine Mode</a><a class="headerlink" href="#engine-mode" title="Permalink to this heading">¶</a></h2>
<p>To ease the engine configuration, the constructors of <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> and
<a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a> have an optional argument <code class="docutils literal notranslate"><span class="pre">mode</span></code>,
which falls into one of the three options <code class="docutils literal notranslate"><span class="pre">&quot;local&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;interactive&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;server&quot;</span></code>.
The default mode is <code class="docutils literal notranslate"><span class="pre">&quot;local&quot;</span></code>.</p>
<p>Each mode denotes a pre-defined configuration of the engine to satisfy different use cases.
The choice of the mode controls the request concurrency of the engine,
as well as engine’s KV cache token capacity (or in other words, the maximum
number of tokens that the engine’s KV cache can hold),
and further affects the GPU memory usage of the engine.</p>
<p>In short,</p>
<ul class="simple">
<li><p>mode <code class="docutils literal notranslate"><span class="pre">&quot;local&quot;</span></code> uses low request concurrency and low KV cache capacity, which is suitable for cases where <strong>concurrent requests are not too many, and the user wants to save GPU memory usage</strong>.</p></li>
<li><p>mode <code class="docutils literal notranslate"><span class="pre">&quot;interactive&quot;</span></code> uses 1 as the request concurrency and low KV cache capacity, which is designed for <strong>interactive use cases</strong> such as chats and conversations.</p></li>
<li><p>mode <code class="docutils literal notranslate"><span class="pre">&quot;server&quot;</span></code> uses as much request concurrency and KV cache capacity as possible. This mode aims to <strong>fully utilize the GPU memory for large server scenarios</strong> where concurrent requests may be many.</p></li>
</ul>
<p><strong>For system benchmark, please select mode</strong> <code class="docutils literal notranslate"><span class="pre">&quot;server&quot;</span></code>.
Please refer to <a class="reference internal" href="#python-engine-api-reference"><span class="std std-ref">API Reference</span></a> for detailed documentation of the engine mode.</p>
</section>
<section id="deploy-your-own-model-with-python-api">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">Deploy Your Own Model with Python API</a><a class="headerlink" href="#deploy-your-own-model-with-python-api" title="Permalink to this heading">¶</a></h2>
<p>The <a class="reference internal" href="../get_started/introduction.html#introduction-deploy-your-own-model"><span class="std std-ref">introduction page</span></a> introduces how we can deploy our
own models with MLC LLM.
This section introduces how you can use the model weights you convert and the model library you build
in <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> and <a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a>.</p>
<p>We use the <a class="reference external" href="https://huggingface.co/microsoft/phi-2">Phi-2</a> as the example model.</p>
<p><strong>Specify Model Weight Path.</strong> Assume you have converted the model weights for your own model,
you can construct a <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">LLMEngine</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;models/phi-2&quot;</span>  <span class="c1"># Assuming the converted phi-2 model weights are under &quot;models/phi-2&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">LLMEngine</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Specify Model Library Path.</strong> Further, if you build the model library on your own,
you can use it in <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> by passing the library path through argument <code class="docutils literal notranslate"><span class="pre">model_lib_path</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mlc_llm</span> <span class="kn">import</span> <span class="n">LLMEngine</span>

<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;models/phi-2&quot;</span>
<span class="n">model_lib_path</span> <span class="o">=</span> <span class="s2">&quot;models/phi-2/lib.so&quot;</span>  <span class="c1"># Assuming the phi-2 model library is built at &quot;models/phi-2/lib.so&quot;</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">LLMEngine</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_lib_path</span><span class="o">=</span><span class="n">model_lib_path</span><span class="p">)</span>
</pre></div>
</div>
<p>The same applies to <a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a>.</p>
</section>
<section id="api-reference">
<span id="python-engine-api-reference"></span><h2><a class="toc-backref" href="#id11" role="doc-backlink">API Reference</a><a class="headerlink" href="#api-reference" title="Permalink to this heading">¶</a></h2>
<p>The <a class="reference internal" href="#mlc_llm.LLMEngine" title="mlc_llm.LLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.LLMEngine</span></code></a> and <a class="reference internal" href="#mlc_llm.AsyncLLMEngine" title="mlc_llm.AsyncLLMEngine"><code class="xref py py-class docutils literal notranslate"><span class="pre">mlc_llm.AsyncLLMEngine</span></code></a> classes provide the following constructors.</p>
<p>The LLMEngine and AsyncLLMEngine have full OpenAI API completeness.
Please refer to <a class="reference external" href="https://github.com/openai/openai-python?tab=readme-ov-file#usage">OpenAI’s Python package</a>
and <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create">OpenAI chat completion API</a>
for the complete chat completion interface.</p>
<dl class="py class">
<dt class="sig sig-object py" id="mlc_llm.LLMEngine">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_llm.</span></span><span class="sig-name descname"><span class="pre">LLMEngine</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'local'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'interactive'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'server'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'local'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additional_models</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_total_sequence_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_memory_utilization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">speculative_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SpeculativeMode</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">SpeculativeMode.DISABLE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spec_draft_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_tracing</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.LLMEngine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LLMEngineBase</span></code></p>
<p>The LLMEngine in MLC LLM that provides the synchronous
interfaces with regard to OpenAI API.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>models</strong> (<em>str</em>) – A path to <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, or an MLC model directory that contains
<cite>mlc-chat-config.json</cite>.
It can also be a link to a HF repository pointing to an MLC compiled model.</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Device</em><em>]</em>) – The device used to deploy the model such as “cuda” or “cuda:0”.
Will default to “auto” and detect from local available GPUs if not specified.</p></li>
<li><p><strong>model_lib_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).
If unspecified, we will use the provided <code class="docutils literal notranslate"><span class="pre">model</span></code> to search over possible paths.
It the model lib path is not found, it will be compiled in a JIT manner.</p></li>
<li><p><strong>mode</strong> (<em>Literal</em><em>[</em><em>&quot;local&quot;</em><em>, </em><em>&quot;interactive&quot;</em><em>, </em><em>&quot;server&quot;</em><em>]</em>) – <p>The engine mode in MLC LLM.
We provide three preset modes: “local”, “interactive” and “server”.
The default mode is “local”.
The choice of mode decides the values of “max_batch_size”, “max_total_sequence_length”
and “prefill_chunk_size” when they are not explicitly specified.
1. Mode “local” refers to the local server deployment which has low
request concurrency. So the max batch size will be set to 4, and max
total sequence length and prefill chunk size are set to the context
window size (or sliding window size) of the model.
2. Mode “interactive” refers to the interactive use of server, which
has at most 1 concurrent request. So the max batch size will be set to 1,
and max total sequence length and prefill chunk size are set to the context
window size (or sliding window size) of the model.
3. Mode “server” refers to the large server use case which may handle
many concurrent request and want to use GPU memory as much as possible.
In this mode, we will automatically infer the largest possible max batch
size and max total sequence length.</p>
<p>You can manually specify arguments “max_batch_size”, “max_total_sequence_length” and
“prefill_chunk_size” to override the automatic inferred values.</p>
</p></li>
<li><p><strong>additional_models</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The model paths and (optional) model library paths of additional models
(other than the main model).
When engine is enabled with speculative decoding, additional models are needed.
Each string in the list is either in form “model_path” or “model_path:model_lib_path”.
When the model lib path of a model is not given, JIT model compilation will
be activated to compile the model automatically.</p></li>
<li><p><strong>max_batch_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The maximum allowed batch size set for the KV cache to concurrently support.</p></li>
<li><p><strong>max_total_sequence_length</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The KV cache total token capacity, i.e., the maximum total number of tokens that
the KV cache support. This decides the GPU memory size that the KV cache consumes.
If not specified, system will automatically estimate the maximum capacity based
on the vRAM size on GPU.</p></li>
<li><p><strong>prefill_chunk_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The maximum number of tokens the model passes for prefill each time.
It should not exceed the prefill chunk size in model config.
If not specified, this defaults to the prefill chunk size in model config.</p></li>
<li><p><strong>gpu_memory_utilization</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – A number in (0, 1) denoting the fraction of GPU memory used by the server in total.
It is used to infer to maximum possible KV cache capacity.
When it is unspecified, it defaults to 0.90.
Under mode “local” or “interactive”, the actual memory usage may be
significantly smaller than this number. Under mode “server”, the actual
memory usage may be slightly larger than this number.</p></li>
<li><p><strong>engine_config</strong> (<em>Optional</em><em>[</em><em>EngineConfig</em><em>]</em>) – The LLMEngine execution configuration.
Currently speculative decoding mode is specified via engine config.
For example, you can use “–engine-config=’spec_draft_length=4;speculative_mode=EAGLE’”
to specify the eagle-style speculative decoding.
Check out class <cite>EngineConfig</cite> in mlc_llm/serve/config.py for detailed specification.</p></li>
<li><p><strong>enable_tracing</strong> (<em>bool</em>) – A boolean indicating if to enable event logging for requests.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.LLMEngine.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'local'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'interactive'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'server'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'local'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additional_models</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_total_sequence_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_memory_utilization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">speculative_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SpeculativeMode</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">SpeculativeMode.DISABLE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spec_draft_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_tracing</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#mlc_llm.LLMEngine.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.LLMEngine.abort">
<span class="sig-name descname"><span class="pre">abort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#mlc_llm.LLMEngine.abort" title="Permalink to this definition">¶</a></dt>
<dd><p>Generation abortion interface.</p>
<dl class="simple">
<dt>request_id<span class="classifier">str</span></dt><dd><p>The id of the request to abort.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="mlc_llm.AsyncLLMEngine">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">mlc_llm.</span></span><span class="sig-name descname"><span class="pre">AsyncLLMEngine</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'local'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'interactive'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'server'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'local'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additional_models</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_total_sequence_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_memory_utilization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">speculative_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SpeculativeMode</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">SpeculativeMode.DISABLE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spec_draft_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_tracing</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#mlc_llm.AsyncLLMEngine" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LLMEngineBase</span></code></p>
<p>The AsyncLLMEngine in MLC LLM that provides the asynchronous
interfaces with regard to OpenAI API.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>models</strong> (<em>str</em>) – A path to <code class="docutils literal notranslate"><span class="pre">mlc-chat-config.json</span></code>, or an MLC model directory that contains
<cite>mlc-chat-config.json</cite>.
It can also be a link to a HF repository pointing to an MLC compiled model.</p></li>
<li><p><strong>device</strong> (<em>Union</em><em>[</em><em>str</em><em>, </em><em>Device</em><em>]</em>) – The device used to deploy the model such as “cuda” or “cuda:0”.
Will default to “auto” and detect from local available GPUs if not specified.</p></li>
<li><p><strong>model_lib_path</strong> (<em>Optional</em><em>[</em><em>str</em><em>]</em>) – The full path to the model library file to use (e.g. a <code class="docutils literal notranslate"><span class="pre">.so</span></code> file).
If unspecified, we will use the provided <code class="docutils literal notranslate"><span class="pre">model</span></code> to search over possible paths.
It the model lib path is not found, it will be compiled in a JIT manner.</p></li>
<li><p><strong>mode</strong> (<em>Literal</em><em>[</em><em>&quot;local&quot;</em><em>, </em><em>&quot;interactive&quot;</em><em>, </em><em>&quot;server&quot;</em><em>]</em>) – <p>The engine mode in MLC LLM.
We provide three preset modes: “local”, “interactive” and “server”.
The default mode is “local”.
The choice of mode decides the values of “max_batch_size”, “max_total_sequence_length”
and “prefill_chunk_size” when they are not explicitly specified.
1. Mode “local” refers to the local server deployment which has low
request concurrency. So the max batch size will be set to 4, and max
total sequence length and prefill chunk size are set to the context
window size (or sliding window size) of the model.
2. Mode “interactive” refers to the interactive use of server, which
has at most 1 concurrent request. So the max batch size will be set to 1,
and max total sequence length and prefill chunk size are set to the context
window size (or sliding window size) of the model.
3. Mode “server” refers to the large server use case which may handle
many concurrent request and want to use GPU memory as much as possible.
In this mode, we will automatically infer the largest possible max batch
size and max total sequence length.</p>
<p>You can manually specify arguments “max_batch_size”, “max_total_sequence_length” and
“prefill_chunk_size” to override the automatic inferred values.</p>
</p></li>
<li><p><strong>additional_models</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><em>str</em><em>]</em><em>]</em>) – The model paths and (optional) model library paths of additional models
(other than the main model).
When engine is enabled with speculative decoding, additional models are needed.
Each string in the list is either in form “model_path” or “model_path:model_lib_path”.
When the model lib path of a model is not given, JIT model compilation will
be activated to compile the model automatically.</p></li>
<li><p><strong>max_batch_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The maximum allowed batch size set for the KV cache to concurrently support.</p></li>
<li><p><strong>max_total_sequence_length</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The KV cache total token capacity, i.e., the maximum total number of tokens that
the KV cache support. This decides the GPU memory size that the KV cache consumes.
If not specified, system will automatically estimate the maximum capacity based
on the vRAM size on GPU.</p></li>
<li><p><strong>prefill_chunk_size</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – The maximum number of tokens the model passes for prefill each time.
It should not exceed the prefill chunk size in model config.
If not specified, this defaults to the prefill chunk size in model config.</p></li>
<li><p><strong>gpu_memory_utilization</strong> (<em>Optional</em><em>[</em><em>float</em><em>]</em>) – A number in (0, 1) denoting the fraction of GPU memory used by the server in total.
It is used to infer to maximum possible KV cache capacity.
When it is unspecified, it defaults to 0.90.
Under mode “local” or “interactive”, the actual memory usage may be
significantly smaller than this number. Under mode “server”, the actual
memory usage may be slightly larger than this number.</p></li>
<li><p><strong>engine_config</strong> (<em>Optional</em><em>[</em><em>EngineConfig</em><em>]</em>) – The LLMEngine execution configuration.
Currently speculative decoding mode is specified via engine config.
For example, you can use “–engine-config=’spec_draft_length=4;speculative_mode=EAGLE’”
to specify the eagle-style speculative decoding.
Check out class <cite>EngineConfig</cite> in mlc_llm/serve/config.py for detailed specification.</p></li>
<li><p><strong>enable_tracing</strong> (<em>bool</em>) – A boolean indicating if to enable event logging for requests.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.AsyncLLMEngine.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Union</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Device</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">model_lib_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Literal</span><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'local'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'interactive'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'server'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'local'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">additional_models</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_total_sequence_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefill_chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gpu_memory_utilization</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Optional</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">speculative_mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">SpeculativeMode</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">SpeculativeMode.DISABLE</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spec_draft_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_tracing</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#mlc_llm.AsyncLLMEngine.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="mlc_llm.AsyncLLMEngine.abort">
<em class="property"><span class="pre">async</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">abort</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">request_id</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span></span></span><a class="headerlink" href="#mlc_llm.AsyncLLMEngine.abort" title="Permalink to this definition">¶</a></dt>
<dd><p>Generation abortion interface.</p>
<dl class="simple">
<dt>request_id<span class="classifier">str</span></dt><dd><p>The id of the request to abort.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ios.html" class="btn btn-neutral float-right" title="iOS App and Swift API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cli.html" class="btn btn-neutral float-left" title="CLI" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>